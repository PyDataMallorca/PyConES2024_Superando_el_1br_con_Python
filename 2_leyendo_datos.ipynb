{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df14d9b1",
   "metadata": {},
   "source": [
    "# Superando el reto del billón de filas con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c56b2",
   "metadata": {},
   "source": [
    "# ¿Qué vamos a hacer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36276e23",
   "metadata": {},
   "source": [
    "* Vamos a empezar leyendo un fichero csv de distintas formas y con distintas bibliotecas.\n",
    "* Vamos a ver si podemos optimizar el espacio en memoria.\n",
    "* Vamos a intentar acelerar esta parte usando todo nuestro hardware.\n",
    "* Vamos a comentar si un csv es lo mejor para almacenar nuestros datos.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import gzip\n",
    "from io import TextIOWrapper\n",
    "import shutil\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import modin.pandas as mpd\n",
    "import dask.dataframe as dd\n",
    "import ibis\n",
    "\n",
    "from src.utils import generate_sample_data, generate_heatmap\n",
    "\n",
    "ibis.options.interactive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7b62a",
   "metadata": {},
   "source": [
    "## Generación de los ficheros CSV que usaremos (si no lo habéis hecho aún)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da49dc",
   "metadata": {},
   "source": [
    "Empezamos creando un fichero más pequeño que el de 1.000.000.000 de filas porque de lo que se trata es de explicar conceptos y no tanto del ***1brc***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4233549",
   "metadata": {},
   "source": [
    "## Generamos CSV (y otros formatos) del tamaño que nos permita nuestro PC (RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb38efb",
   "metadata": {},
   "source": [
    "A la hora de almacenar datos en ficheros es muy habitual utilizar el formato CSV. Es un formato textual que se puede editar fácilmente y además tiene un soporte completo en cualquier herramienta y lenguaje de tratamiento de datos. Cualquier persona que alguna vez haya trabajado con datos se habrá encontrado con ficheros CSV y aunque como veremos quizá no sea el formato más recomendable, sí que es el más usado y por este motivo lo utilizaremos en este tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a8eb3",
   "metadata": {},
   "source": [
    "A modo informativo, el ejercicio original serían 1.000.000.000 de filas por 2 columnas. Supongamos que cada dato en memoria son 64 bits. Eso sería alrededor de 15GiB de memoria. Si en alguna operación se hace alguna copia intermedia ya hablamos de picos de 30GiB, más lo que consuma el sistema operativo y otras cosas que tengáis abiertas.\n",
    "\n",
    "* Si vuestro PC tiene una RAM de 8GiB os recomiendo que no uséis ficheros de más de 50.000.000 o 100.000.000 filas.\n",
    "* Si vuestro PC tiene una RAM de 16GiB os recomiendo que no uséis ficheros de más de 100.000.000 o 200.000.000 filas.\n",
    "* Si vuestro PC tiene una RAM de 32GiB os recomiendo que no uséis ficheros de más de 500.000.000 filas.\n",
    "* Si vuestro PC tiene una RAM de 64 o más GiB debería ser seguro usar ficheros de 1.000.000.000 filas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbcffb",
   "metadata": {},
   "source": [
    "Yo voy a usar ficheros de 50.000.000 de filas para esta primera parte para que muchas cosas no se eternicen y para ir haciendo las cosas con soltura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa77a54",
   "metadata": {},
   "source": [
    "A lo largo de esta parte, si no lo habéis hecho ya, generaremos varios ficheros en varios formatos y con distintos tipos de compresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f367352",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">La siguiente celda es algo que deberéis modificar en caso de que queráis probar otras cosas, usar otros nombres, etc.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('.')\n",
    "(path / 'data').mkdir(mode=0o775, exist_ok=True)\n",
    "path_data = path / 'data'\n",
    "filename_ = \"sample50M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ficheros CSV de 50M de filas. \n",
    "generate_sample_data(fmt='csv', filename=f'{str(path_data)}/{filename_}')\n",
    "generate_sample_data(fmt='csv', filename=f'{str(path_data)}/{filename_}', compression= 'gzip')\n",
    "\n",
    "# Ficheros Parquet de 50M de filas. \n",
    "generate_sample_data(fmt='parquet', filename=f'{str(path_data)}/{filename_}')\n",
    "generate_sample_data(fmt='parquet', filename=f'{str(path_data)}/{filename_}', compression = 'gzip')\n",
    "\n",
    "# Ficheros Feather de 50M de filas.\n",
    "generate_sample_data(fmt='feather', filename=f'{str(path_data)}/{filename_}')\n",
    "# ¿Por qué no generamos el feather.zip?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc78d86",
   "metadata": {},
   "source": [
    "Antes de ejecutar nada voy a crear un *dataframe* para ir añadiendo los resultados de las distintas pruebas y así las podremos ver todas al final y ver qué es lo que nos ha funcionado mejor y lo que nos ha funcionado peor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = pd.DataFrame(\n",
    "    index=('python', 'numpy', 'pandas', 'polars', 'pyarrow', 'duckdb', 'dask', 'modin+dask'),\n",
    "    columns=(\n",
    "        'lee csv (por defecto/engine c)', \n",
    "        'lee csv.zip (por defecto/engine c)',\n",
    "        'lee csv (engine pyarrow)', \n",
    "        'lee csv.zip (engine pyarrow)',\n",
    "        'lee parquet', \n",
    "        'lee parquet comprimido',\n",
    "        'lee feather'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847a05b",
   "metadata": {},
   "source": [
    "## Leo CSV con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20de7e",
   "metadata": {},
   "source": [
    "Vamos a probar con Python puro usando solo herramientas de la biblioteca estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{filename_}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f935c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_csv(filename):\n",
    "    rows = []\n",
    "    with open(path_data / filename) as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        for row in csv_reader:\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def lee_csv_gzip(filename):\n",
    "    rows = []\n",
    "    with gzip.open(path_data / f\"{filename}.gz\", \"rb\") as fzip:\n",
    "        csv_reader = csv.reader(TextIOWrapper(fzip))\n",
    "        for row in csv_reader:\n",
    "            rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681421c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -o -r 3 lee_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5448ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['python', 'lee csv (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -o -r 3 lee_csv_gzip(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['python', 'lee csv.zip (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b340ad",
   "metadata": {},
   "source": [
    "Lo anterior lo hemos metido en una lista de listas. Eso, normalmente, sería poco eficiente para luego hacer las distintas operaciones que necesitemos con los datos. Lo suyo sería meterlo en un `np.array` por lo que también habría que incluir ese tiempo en lo anterior. Lo dejamos así por simplificar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53859f04",
   "metadata": {},
   "source": [
    "Veamos como van los resultados, de momento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a08bf7",
   "metadata": {},
   "source": [
    "* ¿Tiene sentido comprimir los ficheros? ¿Cuándo? ¿Por qué?,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583a7e7",
   "metadata": {},
   "source": [
    "## Leo CSV con numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc90a4",
   "metadata": {},
   "source": [
    "Vamos a probar con `numpy`. Dispone de funcionalidad para leer ficheros pero, en general, no son ninguna maravilla en lo que a rendimiento se refiere. Si hay que abrir un CSV de miles, decenas de miles o, incluso, centenas de miles de filas no es una mala opción ya que el tiempo que necesita no es tanto. Si tenemos que abrir ficheros más grandes quizá sea una mala opción. Pero no nos adelantemos. Hagamos las pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28938ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -o -r 3 np.genfromtxt(path_data / filename, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['numpy', 'lee csv (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -o -r 3 np.genfromtxt(path_data / f\"{filename}.gz\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf98544",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['numpy', 'lee csv.zip (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f1fab",
   "metadata": {},
   "source": [
    "En este caso estamos usando `np.genfromtxt` que ofrece cierta flexibilidad y suele tener mejor rendimiento que `np.loadtxt`. Pero, como hemos comentado antes, `numpy` está usando mucho código python poco optimizado/que hace muchas cosas para la lectura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5567be",
   "metadata": {},
   "source": [
    "Veamos como van los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d31431",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a3369",
   "metadata": {},
   "source": [
    "* ¿Qué podemos comentar de esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484697e",
   "metadata": {},
   "source": [
    "## Leo CSV con pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ab9c9",
   "metadata": {},
   "source": [
    "Con pandas, ¿es más rápido abrir un fichero comprimido o un fichero sin comprimir?, ¿es más rápido que usando las opciones anteriores? Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f970677",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee csv (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d888e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_csv(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee csv.zip (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13d808",
   "metadata": {},
   "source": [
    "`pandas`, por defecto, usa un *motor* de lectura escrito en *C*. Además, dispone de un *motor* escrito en *Python*, que es más lento pero más flexible [1] y, desde la versión 1.4, dispone de un *motor* que usa `arrow` [2].\n",
    "\n",
    "[1] https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas-read-csv\n",
    "\n",
    "[2] https://pandas.pydata.org/docs/user_guide/pyarrow.html#i-o-reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471fe85f",
   "metadata": {},
   "source": [
    "Antes hemos usado la opción por defecto, `engine='c'`. Ahora usaremos `pyarrow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f15b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_csv(path_data / filename, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae260557",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee csv (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_csv(path_data / f\"{filename}.gz\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254911cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee csv.zip (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb3195",
   "metadata": {},
   "source": [
    "Vamos, nuevamente, a ver los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d26071",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d11ea",
   "metadata": {},
   "source": [
    "* ¿Qué hemos visto hasta ahora?\n",
    "\n",
    "* ¿El comprimir el fichero merece la pena?\n",
    "\n",
    "* ¿Es lo mismo usar el *engine* `c` o el *engine* `pyarrow`?\n",
    "\n",
    "* ¿Estamos aprovechando todo nuestro hardware con `pandas`? ¿Con `pandas` + `pyarrow`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ebc39",
   "metadata": {},
   "source": [
    "## Leo CSV con Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcb9ab",
   "metadata": {},
   "source": [
    "Vamos a añadir ahora `Polars` a ver qué tal se comporta. `Polars` es parecido a `pandas` en espíritu pero tiene varias diferencias:\n",
    "\n",
    "* Escrito en Rust.\n",
    "* No usa índices.\n",
    "* Usa `arrow` en lugar de `numpy` para almacenar los datos en memoria.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc17740",
   "metadata": {},
   "source": [
    "¿Probamos a ver qué tal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pl.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['polars', 'lee csv (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a74aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pl.read_csv(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['polars', 'lee csv.zip (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798673e",
   "metadata": {},
   "source": [
    "Eso parece que ha sido rápido. Veamos los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dac3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351fc2b1",
   "metadata": {},
   "source": [
    "`Polars` ha llegado a su versión 1.0. Se considera estable. Por otro lado, también tiene un desarrollo muy rápido aunque todavía no dispone de toda la funcionalidad que tiene `pandas` en este momento. A veces nos puede interesar convertir un *dataframe* de `Polars` a `pandas` para seguir trabajando con `pandas`. Vamos a leer el fichero usando `Polars` y veamos varias cosas interesantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.estimated_size(\"mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd92e9f",
   "metadata": {},
   "source": [
    "Si, por lo que sea, necesitamos usar alguna funcionalidad que está en `pandas` y no en `Polars` podríamos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_pyarrow_extension_array=False es la opción por defecto\n",
    "# Polars nos dará un dataframe de pandas con tipos de python, str, np, etc\n",
    "padf = df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf6b12",
   "metadata": {},
   "source": [
    "Ahora podemos usar, por ejemplo, el método `info` que no está en `Polars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "padf.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b7df2",
   "metadata": {},
   "source": [
    "Aquí vemos dos cosas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f6c77",
   "metadata": {},
   "source": [
    "La primera, al transformarlo a un *dataframe* de `pandas` ha tardado algo de tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37529887",
   "metadata": {},
   "source": [
    "Si queremos leer con `Polars` y luego usar la información para hacer operaciones con `pandas` vemos que tarda casi un segundo en hacer la transformación. Por tanto, puede haber casos en que usar directamente `pandas` para leer sea más eficiente que usar `Polars` para leer y transformar a `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342d9e4",
   "metadata": {},
   "source": [
    "Segundo, vemos los tamaños de ambos *dataframes*. El de `Polars` ocupa bastante menos. `Polars` usa `arrow` en sus tripas y esto se nota. Podemos hacer que `pandas` use `arrow` también:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pandas(use_pyarrow_extension_array=True).info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8e67e",
   "metadata": {},
   "source": [
    "* ¿Qué hemos visto ahora? Dos cositas que acabamos de comentar..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec5030",
   "metadata": {},
   "source": [
    "## Leo CSV con PyArrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17cebc",
   "metadata": {},
   "source": [
    "Hemos visto que podemos usar `PyArrow` para leer ficheros con `pandas`. Hemos visto que `Polars` usa `arrow` para almacenar los datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6539bd",
   "metadata": {},
   "source": [
    "Vamos a usar `PyArrow` para leer el csv a ver lo que nos ofrece sin intermediarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e80449",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pa.csv.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pyarrow', 'lee csv (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e033f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pa.csv.read_csv(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44695f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pyarrow', 'lee csv.zip (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7524e",
   "metadata": {},
   "source": [
    "¿Cómo están ahora los resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b9d98",
   "metadata": {},
   "source": [
    "* ¿Es conveniente usar siempre `Pyarrow`?\n",
    "* ¿Merece la pena el pequeño retraso que introduce `Polars` sobre `PyArrow`?\n",
    "* ¿Es asumible usar `pandas`?\n",
    "* ¿Debemos usar `pandas` usando `numpy` como *backend* o es mejor usar `arrow` como *backend*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9dbe4",
   "metadata": {},
   "source": [
    "## Leo CSV con DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6811c43",
   "metadata": {},
   "source": [
    "DuckDB es una base de datos analítica que se integra perfectamente en Python. Y que además, podemos usar como cualquier otra librería. Vamos a usarla aquí para leer el fichero en formato CSV, comprimido y sin comprimir, y lo metemos en una tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o duckdb.sql(f\"CREATE OR REPLACE TEMPORARY TABLE temp_table AS SELECT * FROM './{str(path_data)}/{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f94018",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['duckdb', 'lee csv (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b90e2c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o duckdb.sql(f\"CREATE OR REPLACE TEMPORARY TABLE temp_table AS SELECT * FROM './{str(path_data)}/{filename}.gz'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['duckdb', 'lee csv.zip (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aecfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe4956",
   "metadata": {},
   "source": [
    "### Pequeño inciso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae701d",
   "metadata": {},
   "source": [
    "Existe una biblioteca que se llama `ibis`. Pretende ser una abstracción sobre muchos *backend*s: PostgreSQL, MySQL, SQLite, Polars,... El *backend* por defecto que usa es `DuckDB` y permite usar todo esto de una forma que se parece más a cómo usamos `pandas` o `Polars`. Entre las ventajas que le veo, lo que comento, que permite cambiar de *backend* sin cambiar apenas el código que usamos y que muchas expresiones se parecen más a `pandas` o `Polars` que a SQL. Vemos un pequeño ejemplo (recordad que por debajo usa `DuckDB` si no cambiamos nada):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibis.read_csv(path_data / filename).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd1640",
   "metadata": {},
   "source": [
    "Reproducimos lo que hemos hecho antes con `DuckDB`, leer el csv y meterlo en una tabla en memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c467aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 3 -o ibis.read_csv(path_data / filename).as_table().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dbc9e",
   "metadata": {},
   "source": [
    "# ¿Qué pasa con el tipo de los datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c8c92",
   "metadata": {},
   "source": [
    "En general, las distintas bibliotecas generalizan al tipo de dato que abarca mayor rango para el valor. Anteriormente hemos visto, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251637b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padf.info())\n",
    "print()\n",
    "print(padf['price'].min(), padf['price'].max())\n",
    "print()\n",
    "print(padf['product'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bd7b9",
   "metadata": {},
   "source": [
    "* ¿Necesitamos usar 'int64' para guardar datos que se encuentran en un rango entre 5 y 114?\n",
    "* ¿Cabrían en otro tipo de dato?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b04ef1",
   "metadata": {},
   "source": [
    "Vamos a adaptar el código de [1] para hacer una pequeña prueba usando `numpy`. No se trata de ver lo eficiente que es `numpy` en hacer una operación concreta sino de entender la implicación de usar algún tipo de dato u otro:\n",
    "\n",
    "[1] https://code.whatever.social/questions/15340781/python-numpy-data-types-performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "setup = \"\"\"\n",
    "import numpy as np\n",
    "A = np.ones((1000,1000,3), dtype=datatype)\n",
    "\"\"\"\n",
    "\n",
    "datatypes = \"np.uint8\", \"np.uint16\", \"np.uint32\", \"np.uint64\",  \"np.float16\", \"np.float32\", \"np.float64\"\n",
    "\n",
    "stmt1 = \"\"\"\n",
    "A = A * 255\n",
    "A = A / 255\n",
    "A = A - 1\n",
    "A = A + 1\n",
    "\"\"\"\n",
    "\n",
    "stmt2 = \"\"\"\n",
    "A *= 255\n",
    "A -= 1\n",
    "A += 1\n",
    "\"\"\"\n",
    "\n",
    "stmt3 = \"\"\"\n",
    "A = A * 255 / 255 - 1 + 1\n",
    "\"\"\"\n",
    "\n",
    "stmt4 = \"\"\"\n",
    "A[:,:,:2] *= A[:,:,:2]\n",
    "\"\"\"\n",
    "\n",
    "stmt5 = \"\"\"\n",
    "A[:,:,:2] = A[:,:,:2] * A[:,:,:2]\n",
    "\"\"\"\n",
    "\n",
    "stmt6 = \"\"\"\n",
    "A *= 4\n",
    "\"\"\"\n",
    "\n",
    "for stmt in [stmt1, stmt2, stmt3, stmt4, stmt5, stmt6]:\n",
    "    print(stmt)\n",
    "    for d in datatypes:\n",
    "        s = setup.replace(\"datatype\", d)\n",
    "        T = timeit.Timer(stmt=stmt, setup=s)\n",
    "        print(d,\":\", min(T.repeat(number=30)))\n",
    "    print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e933420",
   "metadata": {},
   "source": [
    "* ¿Qué conclusiones podemos obtener de los resultados anteriores?\n",
    "* ¿Por qué el `float16` se comporta tan mal comparado con el resto?\n",
    "* ¿Es una buena solución de compromiso usar tipos de datos más pequeños?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934b2b7",
   "metadata": {},
   "source": [
    "Veamos lo que ocuparía nuestro *dataframe* de `pandas` si usásemos `uint8` para la columna de precios y `bytes` para la columna de productos nos quedaría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"con 64 bits en la columna de precios\")\n",
    "print(padf.info(memory_usage='deep'))\n",
    "print()\n",
    "print(\"con 8 bits en la columna de precios\")\n",
    "_padf = pd.DataFrame()\n",
    "_padf['product'] = padf['product'].astype(bytes)\n",
    "_padf['price'] = padf['price'].astype(np.uint8)\n",
    "print(_padf.info(memory_usage='deep'))\n",
    "print()\n",
    "print(_padf.head())\n",
    "del _padf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cálculo del espacio ocupado en MiB: \")\n",
    "print(\n",
    "    (\n",
    "        50_000_000 # bytes para la columna 'product'\n",
    "        + \n",
    "        50_000_000 # bytes para la columna 'price'\n",
    "    )\n",
    "    /\n",
    "    1024 # de bytes a kiB\n",
    "    / \n",
    "    1024 # de kiB a MiB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890760b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuantas veces ocupa de más el df original: \")\n",
    "print(3.1 * 1024 / 95.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091cdcc",
   "metadata": {},
   "source": [
    "* ¿Debemos usar siempre el tipo de dato más pequeño en el que quepa nuestro dato?\n",
    "* ¿Es mejor elegir un tipo más universal si las operaciones son más eficientes?\n",
    "* ¿Dependerá del tipo de operaciones que estemos haciendo?\n",
    "* ¿Qué es el desbordamiento/*overflow*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d41939",
   "metadata": {},
   "source": [
    "A lo hora de leer los datos podemos usar opciones para definir directamente el tipo de los datos que queremos que use en el momento de lectura y no lo tenemos que hacer a posteriori.\n",
    "\n",
    "[1] https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html\n",
    "\n",
    "[2] https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "\n",
    "[3] https://docs.pola.rs/py-polars/html/reference/api/polars.read_csv.html\n",
    "\n",
    "[4] https://arrow.apache.org/docs/python/generated/pyarrow.csv.read_csv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9c120",
   "metadata": {},
   "source": [
    "# Estamos usando de forma eficiente nuestro *hardware* para leer los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26801ea3",
   "metadata": {},
   "source": [
    "Muchas operaciones que hemos hecho hasta ahora no sacan todo el rendimiento de nuestro equipo pero hay formas de exprimir mejor nuestros recursos. Hay bibliotecas que nos permiten hacer esto sin mucho dolor de cabeza. Esto lo vamos a ver por encima, de momento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3d4ce",
   "metadata": {},
   "source": [
    "Usando `Dask` podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dd.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c99ab6",
   "metadata": {},
   "source": [
    "No vemos nada, de momento, porque `Dask` usa evaluación perezosa (*lazy evaluation*). Hasta que no hay que usar algo no hace el trabajo. Si queremos que haga el trabajo de forma explícita lo podemos hacer así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(path_data / filename).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56451ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = dd.read_csv(path_data / filename).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['dask', 'lee csv (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba5002",
   "metadata": {},
   "source": [
    "En la ejecución de `Dask` con un fichero comprimido es interesante leer esa *issue*: https://github.com/dask/dask/issues/2554\n",
    "\n",
    "Para leer el fichero comprimido `Dask` necesita meterlo todo en memoria. y no puede hacer lecturas en paralelo en caso de que hubiera varios ficheros CSV dentro del fichero comprimido. Si no usamos la opción `blocksize=None` nos responderá que el siguiente aviso:\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "UserWarning: Warning zip compression does not support breaking apart files<br>\n",
    "Please ensure that each individual file can fit in memory and<br>\n",
    "use the keyword ``blocksize=None to remove this message``<br>\n",
    "Setting ``blocksize=None``\n",
    "</div>\n",
    "\n",
    "https://docs.dask.org/en/stable/generated/dask.dataframe.read_csv.html#dask.dataframe.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = dd.read_csv(path_data / f\"{filename}.gz\", blocksize=None).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['dask', 'lee csv.zip (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = dd.read_csv(path_data / filename, engine='pyarrow').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['dask', 'lee csv (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = dd.read_csv(path_data / f\"{filename}.gz\", engine='pyarrow', blocksize=None).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['dask', 'lee csv.zip (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8ee12",
   "metadata": {},
   "source": [
    "* ¿Qué podemos comentar de lo anterior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a9b0b",
   "metadata": {},
   "source": [
    "Vamos a hacer lo mismo con `Modin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deaa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = mpd.read_csv(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['modin+dask', 'lee csv (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = mpd.read_csv(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102cd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['modin+dask', 'lee csv.zip (por defecto/engine c)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7720ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = mpd.read_csv(path_data / filename, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b76958",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['modin+dask', 'lee csv (engine pyarrow)'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961898d",
   "metadata": {},
   "source": [
    "* ¿Por qué `Modin` + `Dask` parece funcionar mejor que solo `Dask` (dependiendo del caso)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee026b47",
   "metadata": {},
   "source": [
    "# Usar otros formatos que no sean CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee3a07",
   "metadata": {},
   "source": [
    "El formato CSV tiene algunas limitaciones que hay que tener en cuenta:\n",
    "1. No tiene soporte para la definición del esquema, es decir, de los tipos de datos para cada columna que contiene.\n",
    "2. Relacionado con el punto anterior, no es un formato que permita tener un esquema de datos fácilmente compatible hacia atrás o hacia adelante. Es decir, fácilmente podemos romper la compatibilidad del código que utilice el fichero CSV al que aplicamos algún cambio en sus columnas.\n",
    "3. No existe un estándar claro respecto a cómo deben tratarse caracteres que actúan como separadores de filas o de final de línea.\n",
    "4. No existe forma de definir como tratar valores nulos o *NaN*s.\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42623ce1",
   "metadata": {},
   "source": [
    "Como contraposición a los formatos textuales como CSV existen otros formatos que guardan los datos mediante una codificación diferente, que les permite reducir el espacio ocupado, incluir la posibilidad de definir un esquema de datos, organizar los datos de una forma diferente o incluso permitirte escoger un algoritmo de compresión a voluntad. Son los formatos binarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4865b",
   "metadata": {},
   "source": [
    "## Formato Apache Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e10c0",
   "metadata": {},
   "source": [
    "Entre este tipo de formatos se encuentra Apache Parquet (https://parquet.apache.org). Las principales características de este formato son:\n",
    "1. Los datos contenidos se organizan por columnas y no por filas como en los ficheros CSV. Por ejemplo, esto permite que si necesitamos extraer una columna de un fichero Apache Parquet no sea necesario leer todos los datos como sí ocurre en CSV.\n",
    "2. Además de organizar los datos por columnas, también se estructuran por grupos de filas. Es decir, en un fichero Parquet podemos tener las 100.000 primeras filas en un bloque o grupo en el que se guardan los 100.000 primeros valores de todas las columnas. En estos grupos de filas Parquet añade estadísticas básicas por columna como el valor mínimo, máximo y número de valores NULL que permiten descartar grupos de filas cuando estamos buscando algún valor en concreto.\n",
    "3. También, al organizar los datos por columnas los algoritmos de compresión funcionarán mejor (principalmente si los datos están ordenados por alguna columna), porque será más probable que haya datos similares de forma secuencial, que ayuda a obtener una mayor compresión.\n",
    "4. Y, finalmente, permite codificar dentro de cada fichero el formato o tipo de dato de cada columna.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/2.2/user_guide/io.html#io-parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47bc4c",
   "metadata": {},
   "source": [
    "## Leo Parquet con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e42bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = filename.replace(\"csv\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f83944",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_parquet(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee parquet'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f607ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_parquet(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6104f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee parquet comprimido'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63879ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be34b3b",
   "metadata": {},
   "source": [
    "## Leo Parquet con Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6384246",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pl.read_parquet(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['polars', 'lee parquet'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbe1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pl.read_parquet(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['polars', 'lee parquet comprimido'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a3ff3",
   "metadata": {},
   "source": [
    "## Leo Parquet con PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pa.parquet.read_table(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pyarrow', 'lee parquet'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pa.parquet.read_table(path_data / f\"{filename}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9cd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pyarrow', 'lee parquet comprimido'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ed892",
   "metadata": {},
   "source": [
    "## Leo Parquet con DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = duckdb.sql(f\"CREATE OR REPLACE TEMPORARY TABLE temp_table AS SELECT * FROM './{str(path_data)}/{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc95ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['duckdb', 'lee parquet'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fcb58",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = duckdb.sql(f\"CREATE OR REPLACE TEMPORARY TABLE temp_table AS SELECT * FROM read_parquet('./{str(path_data)}/{filename}.gz')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a769949",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['duckdb', 'lee parquet comprimido'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f1f2c",
   "metadata": {},
   "source": [
    "## Formato Feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c385f",
   "metadata": {},
   "source": [
    "Feather sería una especie de formato arrow en memoria pero replicado en disco. Se supone que los tiempos de escritura y de lectura son más rápidos que con otros formatos pero, por contra, ocupa bastante más espacio en disco.\n",
    "\n",
    "Lo de \"se supone\" es lo que os vais a llevar de esta parte de la charla. Al final del día los resultados dependen mucho del cómo y dónde los estemos procesando.\n",
    "\n",
    "Otra desventaja de Feather con respecto a Parquet es que Parquet está mucho mejor soportado en muchos entornos y es un estándar de facto entre muchas aplicaciones para intercambiar información de forma eficiente y está mejor pensado para ser un formato de almacenamiento de datos duradero.\n",
    "\n",
    "https://arrow.apache.org/docs/python/feather.html\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html\n",
    "\n",
    "https://philipmay.org/blog/2024/pandas-data-format-and-compression.html\n",
    "\n",
    "https://ao.vern.cc/questions/48083405/what-are-the-differences-between-feather-and-parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d02d43",
   "metadata": {},
   "source": [
    "## Leo Feather con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = filename.replace(\"parquet\", \"feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbccb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pd.read_feather(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173be4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pandas', 'lee feather'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a57913",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a30cf",
   "metadata": {},
   "source": [
    "## Leo Feather con Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba8e5f",
   "metadata": {},
   "source": [
    "Como el fichero Feather está comprimido tenemos un comportamiento 'parecido' al comentado anteriormente con `Dask` y nos aparecerá un *warning*, En este caso lo dejo para que se vea de forma explícita pero para eliminarlo podéis usar la opción `memory_map=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pl.read_ipc(path_data / filename)\n",
    "# kk = %timeit -r 3 -o df = pl.read_ipc(path_data / filename, memory_map=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['polars', 'lee feather'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56fdef",
   "metadata": {},
   "source": [
    "## Leo Feather con PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pa.feather.read_feather(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.loc['pyarrow', 'lee feather'] = kk.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ba6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ca652",
   "metadata": {},
   "source": [
    "Con `PyArrow` podemos usar `read_feather` y lo leído lo convierte directamente en un *dataframe* de `pandas` o `read_table` y lo convierte en una tabla de `PyArrow`. En realidad, el primero usa el segundo y luego lo convierte en un *dataframe* de `pandas`. Debido a esto se tarda un poco más en leer que usando directamente `read_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = %timeit -r 3 -o df = pa.feather.read_table(path_data / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acf9de",
   "metadata": {},
   "source": [
    "Finalmente, representaremos los datos obtenidos para visualizar de manera más clara la comparación entre todos los resultados de esta sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7790a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(resultados.transpose(),'Heatmap de los tiempos de carga en memoria principal (seg.).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399bd1b",
   "metadata": {},
   "source": [
    "## Apéndice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f4677",
   "metadata": {},
   "source": [
    "Aquí mostramos tiempos del código anterior ejecutado en otros PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695380ae",
   "metadata": {},
   "source": [
    "---\n",
    "SO: linux (Kernel: 5.15.0-122-generic x86_64)\n",
    "\n",
    "Procesador: AMD Ryzen 9 3900X (cache: L1: 768 KiB L2: 6 MiB L3: 64 MiB)\n",
    "\n",
    "HDD: Crucial model: CT1000BX500SSD1 size: 931.51 GiB speed: 6.0 Gb/s type: SSD serial\n",
    "\n",
    "RAM: 16GiB DIMM DDR4 Speed 2666 MT/s (x 4) = 64 GiB\n",
    "\n",
    "![Resultados](./images/resultados_kiko_desktop_linux_leyendo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5de8c",
   "metadata": {},
   "source": [
    "---\n",
    "SO: Windows 10 Enterprise (64 bits)\n",
    "\n",
    "Procesador: 13th Gen Intel(R) Core(TM) i7-1365U\n",
    "\n",
    "HDD: NVMe CL4-3D512-Q11 NVMe SSSTC 512GB\n",
    "\n",
    "RAM: 2 x 8 GB 3200 MHz DDR4-SDRAM = 16 GB \n",
    "\n",
    "![Resultados](./images/resultados_kiko_pccurr_windows_leyendo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f86da5",
   "metadata": {},
   "source": [
    "---\n",
    "SO: macOS Sequoia (15.0)\n",
    "\n",
    "Procesador: Apple M2 con 8 núcleos\n",
    "\n",
    "HDD: APPLE SSD AP0512Z 512 GB\n",
    "\n",
    "RAM: LPDDR5 16 GB\n",
    "\n",
    "![Resultados](./images/resultados_jordi_mba_leyendo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060fbbf",
   "metadata": {},
   "source": [
    "---\n",
    "SO: Windows 11 Pro 2 (64 bits)\n",
    "\n",
    "Procesador: 14th Intel(R) Core(TM) i7-14700HX  \n",
    "\n",
    "HDD: NVMe™ TLC M.2 de 1 TB SSD \n",
    "\n",
    "RAM: 32 GB de RAM DDR5-4800 MHz (2 x 16 GB)\n",
    "\n",
    "![Resultados](./images/resultados_ernesto_windows_leyendo.png)"
   ]
  }
 ],
 "metadata": {
    "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
